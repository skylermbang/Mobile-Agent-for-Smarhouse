
import json
# preprocess_data -s -m --dur=4.0 -r=44100
# train_network.py --epochs=10 --val=0
settings_json = json.dumps([
    {'type': 'title',
     'title': 'Sorting Hat Settings'},
    {'type': 'string',
     'title': 'Server',
     'desc': 'Hostname or IP address of GPU compute server',
     'section': 'network',
     'key': 'server'},
    {'type': 'string',
     'title': 'Username',
     'desc': 'Username on GPU server',
     'section': 'network',
     'key': 'username'},
    {'type': 'path',
     'title': 'SSH keys',
     'desc': 'Path to ssh keys',
     'section': 'network',
     'key': 'sshKeyPath'},
    {'type': 'bool',
     'title': 'Clean',
     'desc': 'Dataset is known to be completely uniform. (Overrides duration & mono)',
     'section': 'preproc',
     'key': 'clean'},
    {'type': 'bool',
     'title': 'Mono',
     'desc': 'Force audio to mono',
     'section': 'preproc',
     'key': 'mono'},
    {'type': 'bool',
     'title': 'Sequential split',
     'desc': 'For Test/Train split: preserve sequential order of audio clips (False = Shuffle)',
     'section': 'preproc',
     'key': 'sequential'},
    {'type': 'numeric',
     'title': 'Duration',
     'desc': 'Length in seconds of audio clips (truncated/padded if too long/short)',
     'section': 'preproc',
     'key': 'duration'},
    {'type': 'numeric',
     'title': 'Sample rate',
     'desc': 'Sample rate to re-sample all audio clips to',
     'section': 'preproc',
     'key': 'sampleRate'},
    {'type': 'options',
     'title': 'Spectrogram file format ',
     'desc': "File format for spectrogram images. (npz = default)",
     'section': 'preproc',
     'key': 'specFileFormat',
     'options': ['npz', 'png', 'jpeg','npy']},
    {'type': 'bool',
     'title': "Split long files (doesn't work yet)",
     'desc': 'Audio files longer than <duration> get multiple parts sent (False=Truncate)',
     'section': 'preproc',
     'key': 'split_audio'},
    {'type': 'options',
     'title': 'Starting weights',
     'desc': "Initial neural network weights for training (Default = whatever's on the hard drive)",
     'section': 'train',
     'key': 'weightsOption',
     'options': ['Default', 'Random', "Upload (doesn't work yet)"]},
    {'type': 'numeric',
     'title': 'Epochs',
     'desc': 'Number of machine learning iterations to run.',
     'section': 'train',
     'key': 'epochs'},
    {'type': 'numeric',
     'title': 'Validation split',
     'desc': 'Fraction of training data to "split off" for validation',
     'section': 'train',
     'key': 'val_split'},
     {'type': 'numeric',           # this isn't technically training, but we want it to be post-upload
      'title': "Augment factor (doesn't work yet)",
      'desc': 'Squeeze & stretch spectrograms to increase dataset by this factor (1=Off)',
      'section': 'train',
      'key': 'aug_fac'},
     ])
